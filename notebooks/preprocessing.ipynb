{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the ToxPost data\n",
    "\n",
    "In this notebook we will guide you to how the corpus of comments for the ToxPost project was preprocessed. One can divide this preprocessing phase into the following steps:\n",
    "```\n",
    "1. cleaning each comment: removing numbers, links, stopwords, hyphenation, non ascii-characters etc.  \n",
    "2. using TfIdf to shrink each comment to a length of at most 60\n",
    "3. embedding each word in a comment in 100d space using GloVe\n",
    "4. applying pca to in turn embed each word into 25d space\n",
    "5. apply the necessary padding\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current working directory is: /Users/Louis/ml_projects/ToxPost\n"
     ]
    }
   ],
   "source": [
    "# We begin by changing the current working directory:\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"the current working directory is: {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python modules:\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary ToxPost modules:\n",
    "from src.load import load_data\n",
    "from src.features.clean_corpus import clean_corpus\n",
    "from src.features.shrink_corpus import shrink_corpus\n",
    "from src.features.embed_corpus import embed_corpus\n",
    "from resources.glove.load import load_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the raw data (a corpus of 150.000 Youtube comments) and splitting it into __features__ and __labels__ each of which consists of 6 binary values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"./data/raw/data.csv\"\n",
    "\n",
    "# the raw data:\n",
    "raw_data = load_data(raw_data_path, header=True, id=True)\n",
    "# the assciated raw features:\n",
    "raw_features = [datapoint[0] for datapoint in raw_data]\n",
    "#the associated labels:\n",
    "labels = [datapoint[1] for datapoint in raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the preprocessing, we first build a corpus of 10 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a perfectly normal way to write. It should be god(s) with a small g though.\n",
      "\n",
      "\n",
      "\" baktun is NOT a period of \"\"years\"\" ... POD ... it is theorized or understood to be 400-tun or 20-katun. please verify content!!! ~M~ 67.164.145.1 \"\n",
      "\n",
      "\n",
      "*blushes* Thank you very much for the barnstar and your guidance. Your kind words are very much appreciated.-\n",
      "\n",
      "\n",
      "\" If \"\"assume good faith\"\" applies anywhere, it applies here.\"\n",
      "\n",
      "\n",
      "The article was just fine as it was, stop changing it\n",
      "\n",
      "\n",
      "wtf you temp block me for?\n",
      "\n",
      "\n",
      "Why deletion that arictle?????\n",
      "\n",
      "\n",
      "\" — Preceding unsigned comment added by (talk • contribs) \"\n",
      "\n",
      "\n",
      "did not realize that he had only spoken with representatives of the Tokugawa Shogun\n",
      "\n",
      "\n",
      "Thank you, I am working on getting the Town website corrected. Hopefully I can finally get this fixed after the election.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices = [index for index in random.sample(range(0,len(raw_data)),10)]\n",
    "example_corpus = [raw_features[index] for index in indices]\n",
    "\n",
    "for comment in example_corpus:\n",
    "    print(\" \".join(comment))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we clean up each comment in the corpus by:  \n",
    "\n",
    "* removing numbers\n",
    "* removing links\n",
    "* removing punctuation\n",
    "* replacing certain words according to a custom replacement list (to handle typos)\n",
    "* removing stopwords\n",
    "* removing extra whitespaces\n",
    "* removing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 658.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the comment \n",
      "\n",
      "It is a perfectly normal way to write. It should be god(s) with a small g though.\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "perfectly normal way write gods small though\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "\" baktun is NOT a period of \"\"years\"\" ... POD ... it is theorized or understood to be 400-tun or 20-katun. please verify content!!! ~M~ 67.164.145.1 \"\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "baktun period years pod theorized understood tun katun please verify content\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "*blushes* Thank you very much for the barnstar and your guidance. Your kind words are very much appreciated.-\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "blushes thank much barnstar guidance kind words much appreciated\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "\" If \"\"assume good faith\"\" applies anywhere, it applies here.\"\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "assume good faith applies anywhere applies\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "The article was just fine as it was, stop changing it\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "article fine stop changing\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "wtf you temp block me for?\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "wtf temp block\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "Why deletion that arictle?????\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "deletion arictle\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "\" — Preceding unsigned comment added by (talk • contribs) \"\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "— preceding unsigned comment added talk contribs\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "did not realize that he had only spoken with representatives of the Tokugawa Shogun\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "realize spoken representatives tokugawa shogun\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "Thank you, I am working on getting the Town website corrected. Hopefully I can finally get this fixed after the election.\n",
      "\n",
      "was cleaned to \n",
      "\n",
      "thank working getting town website corrected hopefully finally get fixed election\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean the example corpus\n",
    "cleaned_corpus = clean_corpus(example_corpus)\n",
    "for i in range(0,10):\n",
    "    print(\"the comment \\n\\n{}\\n\\nwas cleaned to \\n\\n{}\\n\\n\\n\".format(\" \".join(example_corpus[i]), \" \".join(cleaned_corpus[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shrink the length of each comment.  \n",
    "\n",
    "To do this, we first compute the TfIdf matrix of the corpus. Recall that this produces a matrix \\\\(M\\\\) whose \\\\((i,j)\\\\) entry is the number of times word \\\\(j\\\\) appears in comment i divided by the number of comments word \\\\(j\\\\) appears in.  \n",
    "\n",
    "As a second step, for each comment, we order the words according to their top TfIdf score and keep only the top ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 772.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the comment \n",
      "\n",
      "perfectly normal way write gods small though\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "perfectly normal way write gods small though\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "baktun period years pod theorized understood tun katun please verify content\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "baktun period years pod theorized understood tun katun please verify content\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "blushes thank much barnstar guidance kind words much appreciated\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "blushes thank much barnstar guidance kind words much appreciated\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "assume good faith applies anywhere applies\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "assume good faith applies anywhere applies\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "article fine stop changing\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "article fine stop changing\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "wtf temp block\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "wtf temp block\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "deletion arictle\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "deletion arictle\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "— preceding unsigned comment added talk contribs\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "preceding unsigned comment added talk contribs\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "realize spoken representatives tokugawa shogun\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "realize spoken representatives tokugawa shogun\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "thank working getting town website corrected hopefully finally get fixed election\n",
      "\n",
      "was shrunken to \n",
      "\n",
      "thank working getting town website corrected hopefully finally get fixed election\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# shrink each comment to size 20:\n",
    "shrunken_corpus = shrink_corpus(cleaned_corpus, 20)\n",
    "for i in range(0,10):\n",
    "    print(\"the comment \\n\\n{}\\n\\nwas shrunken to \\n\\n{}\\n\\n\\n\".format(\" \".join(cleaned_corpus[i]), \" \".join(shrunken_corpus[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to embed each comment into a vector space.  \n",
    "To do this, we load the pretrained [GloVe embedding](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "which embeds each word in the vocabulary into \\\\(\\mathbb{R}^{25}\\\\)  \n",
    "\n",
    "After that, we apply pca to reduce the embedding space to \\\\(\\mathbb{R}^{25}\\\\) \n",
    "\n",
    "Finally, for each comment, we simply replace the word with its embedding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21642.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the comment \n",
      "\n",
      "perfectly normal way write gods small though\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([-0.24193238, -0.03976884,  1.47509678,  0.97681112, -1.93304001,\n",
      "        0.02535983,  0.69377276,  0.41232793,  0.43330005,  0.11392516]), array([-0.40167002,  0.97490409, -1.46951848,  1.54668204, -0.36695469,\n",
      "        1.87608986,  0.26783316, -0.16639932, -0.57314258, -0.01347903]), array([-2.09550734,  0.65870088, -0.04530984, -0.51514199, -0.17846698,\n",
      "        0.52045516,  0.06560927,  0.06956183,  0.53142837,  0.20438685]), array([-1.25724584, -1.60114076,  0.29954597,  0.09081673, -0.12156692,\n",
      "       -0.0947948 , -0.56225194,  0.42476465, -0.01721878, -0.53428182]), array([ 0.37628384, -0.18310109,  1.05691994, -1.15116073,  0.44885865,\n",
      "        1.05957712,  1.206314  ,  0.39845276,  0.26111729, -0.30816421]), array([-0.65253115,  0.41351668, -0.19612429, -0.50335689, -1.42096687,\n",
      "        0.16513254,  0.48909643, -0.07045761,  1.16277152,  0.11439013]), array([-2.17196824,  0.73884306,  0.44868645, -0.093335  ,  0.0909535 ,\n",
      "       -0.14125101, -0.21206088,  0.34484451, -0.26692561, -0.22920228])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "baktun period years pod theorized understood tun katun please verify content\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([ 4.31660992,  0.99327134, -0.29243674, -0.8684216 ,  2.14991201,\n",
      "        0.19113877,  0.63237344,  0.13894125, -0.6225341 , -0.54930585]), array([-0.57963186,  0.78166031,  0.18306399, -0.70561634, -0.32604939,\n",
      "        0.62573423, -1.24645721, -0.01832951, -0.26396413, -0.61361019]), array([-1.35539751,  0.47365631,  0.49731115, -1.67257898,  0.04108326,\n",
      "       -0.09932521,  0.21281152, -0.26947431, -0.59952847, -0.85681982]), array([ 1.65973589,  0.63898879, -0.62779054,  0.97034777,  0.27647349,\n",
      "        0.31471399,  0.85740915, -0.05908866,  0.75136326,  0.49440732]), None, array([-0.17330687, -0.97332838,  1.65980179,  0.21761686,  0.27191753,\n",
      "        0.58770389, -0.15875179,  1.24882967, -0.24491648,  0.78876588]), array([ 1.83804747,  1.67023873, -1.12859514,  1.75636587,  0.24467547,\n",
      "       -0.28066346,  0.30060652,  0.8103752 , -1.22768469,  2.04349234]), array([ 3.88111883,  1.48493045, -1.59450287,  1.28051945, -1.48618435,\n",
      "       -1.09852051,  1.03037348,  0.38062552,  1.40190153, -1.17827877]), array([-2.17967153, -0.21608837, -1.07272961,  0.96356521,  1.57000122,\n",
      "       -1.31564668,  0.90236381, -0.99620717,  0.52835188, -0.20328131]), array([ 1.41543451, -1.25340438, -0.58634519,  1.43387755,  2.12827255,\n",
      "       -1.75387336,  0.39756235,  0.71827859,  0.86850346, -0.25321581]), array([ 0.37862568, -2.25407186, -1.60589302,  0.16213014, -1.67649894,\n",
      "        0.65090805,  0.33541521,  0.04127215,  0.02245263,  0.4858162 ])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "blushes thank much barnstar guidance kind words much appreciated\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([ 1.14116489,  1.11396636,  2.6591836 ,  2.88716018, -0.49644854,\n",
      "       -0.79982002,  0.08004636, -1.48859563,  0.21352622, -0.67777601]), array([-1.98052542,  0.19703115,  0.18981849,  0.01439797,  1.05276839,\n",
      "       -0.93802359,  1.66809644, -1.05656168, -0.71163371, -0.14129674]), array([-2.60248936,  0.30842582,  0.74463937,  0.1371867 ,  0.26664395,\n",
      "       -0.19535753,  0.99857986, -0.11340832,  0.38287105, -0.21285923]), None, array([ 1.38136705, -1.59052703,  0.44394756, -1.01809527,  0.08292591,\n",
      "        0.43863337,  0.57664116, -1.23653981, -0.11932514,  1.31610114]), array([-1.26876637, -0.35582528,  0.51965984,  0.36685139, -1.11006827,\n",
      "        0.7876301 ,  1.08988103,  1.33977937, -0.69077063, -0.18142628]), array([-1.34096721, -0.92348521,  1.22416359,  0.06506075, -0.08374703,\n",
      "        1.1500759 ,  0.41766729,  0.47983973,  0.34579697, -1.05639345]), array([-2.60248936,  0.30842582,  0.74463937,  0.1371867 ,  0.26664395,\n",
      "       -0.19535753,  0.99857986, -0.11340832,  0.38287105, -0.21285923]), array([-0.07223933, -1.11419136,  1.07850863,  0.20454299,  0.78571992,\n",
      "       -1.0399663 ,  1.48499824, -0.26066936, -0.22109284,  0.78191831])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "assume good faith applies anywhere applies\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([-0.09258193, -0.74323846, -0.51896607,  1.18607258,  1.08661864,\n",
      "        1.85759387, -1.12024228, -0.24965525,  0.91522032,  0.3246234 ]), array([-2.63585765,  0.943952  , -0.17614517, -0.67900193,  0.06368346,\n",
      "       -0.07301406,  1.10012866, -0.05240452, -0.32698353, -0.15433224]), array([-0.19841844, -0.51800262,  1.18870393, -0.96701126,  0.66938917,\n",
      "        1.40095758,  1.57003521, -0.49966329,  0.64599612,  0.08389531]), array([ 1.13190592, -1.16226467,  1.146849  ,  0.53150192, -0.35227673,\n",
      "       -0.26231476, -0.2337262 , -0.31139676,  0.63718848,  0.08287018]), array([-0.78175556,  0.31251177,  0.00193261,  0.12451768, -0.2255008 ,\n",
      "       -0.50134418, -0.38147354,  0.0993957 ,  0.72249095,  1.35823534]), array([ 1.13190592, -1.16226467,  1.146849  ,  0.53150192, -0.35227673,\n",
      "       -0.26231476, -0.2337262 , -0.31139676,  0.63718848,  0.08287018])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "article fine stop changing\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([ 0.24837269, -2.14809762, -1.22147945, -0.33917666, -1.12772872,\n",
      "       -0.04646246,  0.26388548, -0.04328462, -1.37314944, -0.30793009]), array([-1.09539267,  0.97809062, -0.47247255,  0.87939789,  0.21653455,\n",
      "        0.54405854,  0.15569714, -0.58098501, -0.52833409,  0.27426526]), array([-1.99870636,  0.19624037, -0.72690777,  0.70376757,  0.81331858,\n",
      "        0.65968468, -1.15006697,  0.29133687,  0.16595592,  0.15111152]), array([-0.44269722, -0.6004765 ,  0.25544379, -0.29125984, -0.92144108,\n",
      "        0.31438085, -0.68510948, -0.20024913,  1.13971932,  0.12878762])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "wtf temp block\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([-1.45570868,  0.92507873, -0.26627416,  1.29873502,  0.71308454,\n",
      "        0.4667891 , -0.73693353,  0.97549469, -1.2893019 , -0.50098717]), array([ 1.85601314,  1.57449584, -1.28015227, -0.50254958, -1.35240437,\n",
      "        1.49728026,  0.85965507, -1.15341699, -0.79436909, -0.23395086]), array([-0.16161205,  0.82828943, -1.43366712,  0.46161524,  0.12951618,\n",
      "        0.29264927, -1.28243856, -0.38281145,  0.59575583, -0.50228331])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "deletion arictle\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([ 3.57033475, -1.29675726,  0.7266182 ,  0.37394444, -0.03824114,\n",
      "       -0.46811031, -0.924058  , -0.51318622, -0.52456767, -0.12432129]), None]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "preceding unsigned comment added talk contribs\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([ 3.25300123, -0.88494435,  1.45814524, -0.48480172,  0.04506337,\n",
      "       -0.02269311, -0.63729391, -0.13295958, -0.44053777, -0.22756608]), array([ 2.07460578, -0.85383283, -1.43484549, -1.77955442,  0.62251561,\n",
      "       -0.76310487,  0.37334605,  2.11437247,  0.42988049, -0.33333223]), array([-0.35225641, -1.68935467, -2.30384947,  1.75864969,  0.49301616,\n",
      "        1.19809501, -0.12597364, -0.55929109, -0.37674794, -0.45437582]), array([ 0.00745764, -0.26903884, -0.13159091, -0.30186576, -1.22886972,\n",
      "       -1.36840503,  0.1527774 ,  0.1377089 , -0.21582268, -0.56167627]), array([-2.28908022, -0.19009575, -0.46118816, -0.17195917,  0.86028499,\n",
      "        0.32545364, -0.68700291,  0.59174912,  0.45882407,  0.1829936 ]), None]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "realize spoken representatives tokugawa shogun\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([-1.66839855, -0.27983718,  0.99103956,  0.17142834,  0.43665903,\n",
      "        0.95931328, -0.57941343, -0.09575129,  1.06984956,  0.10200443]), array([ 0.41170047, -1.07073644,  1.71190176, -0.27139278,  0.29207678,\n",
      "        0.47012456,  0.27538475,  0.90470805, -0.87374887, -0.14803466]), array([ 2.34473646, -0.87871492, -0.68823188, -1.34374878,  0.37262192,\n",
      "       -0.63415944, -0.76547986, -1.15192721,  0.54999285,  0.78123456]), array([ 4.44940451,  2.49909899,  1.82102218, -1.20243536, -0.23014858,\n",
      "        0.73267048, -0.33181083,  0.20954577,  0.27514414,  0.56857275]), array([ 3.21575166,  1.56683165, -0.47505264, -0.05101333,  0.55248143,\n",
      "        0.09147969, -0.23600352,  1.15780497,  0.15471592, -0.54011588])]\n",
      "\n",
      "\n",
      "\n",
      "the comment \n",
      "\n",
      "thank working getting town website corrected hopefully finally get fixed election\n",
      "\n",
      "was embedded to \n",
      "\n",
      "[array([-1.98052542,  0.19703115,  0.18981849,  0.01439797,  1.05276839,\n",
      "       -0.93802359,  1.66809644, -1.05656168, -0.71163371, -0.14129674]), array([-1.42845231,  0.11634854, -0.2109936 , -1.0682683 , -0.73277266,\n",
      "       -0.5495467 , -0.67296577,  0.11832782,  0.11025135,  0.77687302]), array([-2.22066   ,  1.02466505,  0.4943091 , -0.68534081, -0.51635213,\n",
      "       -1.12327554, -1.00331224,  0.52273661,  0.10389562, -0.02518753]), array([-0.55731877,  1.6494091 , -1.16622031, -1.41956926,  0.18590704,\n",
      "       -0.17157018, -0.21615463,  0.3089915 , -0.0264746 ,  0.7456448 ]), array([ 0.05805853, -1.87558533, -1.91729491, -0.29644454, -1.39143129,\n",
      "       -1.36221511,  0.1411566 ,  0.24825052, -0.38825936,  0.14020938]), array([ 1.18453449, -1.05125696,  1.6866489 ,  1.34999605,  0.17533127,\n",
      "       -0.62829526, -1.60285183,  0.27912741, -0.8481969 , -0.0818319 ]), array([-1.2956705 ,  0.90539179,  0.24357111, -0.51078677,  0.26901151,\n",
      "       -0.86934924, -0.18599681, -0.88364096, -0.57563428,  0.41865236]), array([-1.55240183,  1.02983522,  0.33777906, -0.48947665, -0.0789462 ,\n",
      "       -1.25885552, -0.38472473, -0.47816578, -1.00266173, -0.21958459]), array([-2.82831012,  0.98422306, -0.27565238, -0.54687766,  0.21104655,\n",
      "       -0.98237974, -0.31995245,  0.42811805,  0.53959941, -0.0480841 ]), array([ 0.21774442,  0.66033684,  0.29521886,  0.16328725, -1.07142676,\n",
      "       -0.81083833, -1.18754084, -0.68976993, -0.181953  , -0.08135674]), array([ 1.01711991, -0.625766  , -1.05930045, -2.14660502,  0.84916553,\n",
      "        0.44949269, -0.96947006, -1.48127115, -0.09238456, -0.90883524])]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embed the comments into 25d space using the GloVe embedding:\n",
    "embedding_path = \"./resources/glove/glove.twitter.27B.25d.txt\"\n",
    "embedding = load_embedding(embedding_path)\n",
    "#Apply pca to reduce the embedding space into 10d using pca\n",
    "dim = 10\n",
    "embedded_corpus = embed_corpus(shrunken_corpus, embedding, dim)\n",
    "for i in range(0,10):\n",
    "    print(\"the comment \\n\\n{}\\n\\nwas embedded to \\n\\n{}\\n\\n\\n\".format(\" \".join(shrunken_corpus[i]), embedded_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we apply the above preprocessing procedures to the whole corpus and write the results in the appropriate data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..loading the data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 70/159570 [00:00<04:26, 597.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..using NLP to clean each comment in the corpus..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159570/159570 [03:20<00:00, 796.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..writing the results to the cleaned data file\n",
      "..using TfIdf to shrink each comment in the corpus to size 100..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159570/159570 [03:57<00:00, 671.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..writing the results to the shrunken data file..\n",
      "\n",
      "..loading the Glove embedding..\n",
      "..using PCA to reduce the embedding space of each word to size 20..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159570/159570 [00:02<00:00, 59550.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..writing the results to the embedded data file. (this takes a while)..\n",
      "\n",
      "..data preprocesssed..\n"
     ]
    }
   ],
   "source": [
    "# finally, we apply the above preprocessing procedures to the whole corpus and write the results in the appropriate data files\n",
    "import src.features.preprocess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
